{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL GPU Scheduling - RELAXED DEADLINES VERSION\n",
    "## Where RL Can Actually Learn Useful Policies\n",
    "\n",
    "---\n",
    "\n",
    "### Problem with Original Setup\n",
    "\n",
    "The original deadline formula `uniform(1.0, 1.5) * g7_duration` is **too tight**:\n",
    "- ~85-90% of jobs are late NO MATTER what scheduling policy is used\n",
    "- Simple heuristics (Largest-First) perform as well as or better than RL\n",
    "- No room for RL to learn interesting trade-offs\n",
    "\n",
    "### This Version Uses Relaxed Deadlines\n",
    "\n",
    "New formula: `uniform(2.0, 4.0) * g7_duration`\n",
    "- Jobs have 100-300% slack time instead of 0-50%\n",
    "- Expected late rate: ~20-40% instead of ~85-90%\n",
    "- RL can learn to balance tardiness vs energy\n",
    "\n",
    "---\n",
    "\n",
    "### Expected Results\n",
    "\n",
    "| Method | Expected Late % | vs Original |\n",
    "|--------|-----------------|-------------|\n",
    "| RL-PPO | 15-25% | Better than heuristics |\n",
    "| Largest-First | 25-35% | Good baseline |\n",
    "| Smallest-First | 35-50% | Worse |\n",
    "| Random | 40-55% | Worst |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q stable-baselines3 sb3-contrib gymnasium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import warnings\n",
    "from typing import Optional, List\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.callbacks import BaseCallback, CallbackList\n",
    "from sb3_contrib.ppo_mask import MaskablePPO\n",
    "from sb3_contrib.common.wrappers import ActionMasker\n",
    "from sb3_contrib.common.maskable.utils import get_action_masks\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Device: {DEVICE}')\n",
    "if DEVICE == 'cuda':\n",
    "    print(f'GPU: {torch.cuda.get_device_name(0)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "MIG_PROFILE = {\n",
    "    1: [(7, 40)], 2: [(4, 20), (3, 20)], 3: [(4, 20), (2, 10), (1, 10)],\n",
    "    4: [(4, 20), (1, 5), (1, 5), (1, 5)], 5: [(3, 20), (3, 20)],\n",
    "    6: [(3, 20), (2, 10), (1, 10)], 7: [(3, 20), (1, 10), (1, 5), (1, 5)],\n",
    "    8: [(2, 10), (2, 10), (3, 20)], 9: [(2, 10), (1, 5), (1, 5), (3, 20)],\n",
    "    10: [(1, 5), (1, 5), (2, 10), (3, 20)], 11: [(1, 5), (1, 5), (1, 5), (1, 5), (3, 20)],\n",
    "    12: [(2, 10), (2, 10), (2, 10), (1, 10)], 13: [(2, 10), (1, 5), (1, 5), (2, 10), (1, 10)],\n",
    "    14: [(1, 5), (1, 5), (2, 10), (2, 10), (1, 10)], 15: [(2, 10), (1, 10), (1, 5), (1, 5), (1, 5), (1, 5)],\n",
    "    16: [(1, 5), (1, 5), (2, 10), (1, 10), (1, 5), (1, 5)],\n",
    "    17: [(1, 5), (1, 5), (1, 10), (1, 5), (2, 10), (1, 5)],\n",
    "    18: [(1, 5), (1, 5), (1, 10), (1, 5), (1, 5), (2, 10)],\n",
    "    19: [(1, 5), (1, 5), (1, 5), (1, 5), (1, 5), (1, 5), (1, 5)]\n",
    "}\n",
    "\n",
    "ENERGY_TABLE = np.array([40, 120, 160, 200, 240, 250, 250, 250], dtype=np.float32)\n",
    "SLICE_DUR_IDX = {1: 2, 2: 3, 3: 4, 4: 5, 7: 6}\n",
    "INTERARRIVALS = np.array([0.111, 0.083, 0.085, 0.1, 0.137, 0.169, 0.171, 0.169, 0.179, 0.191,\n",
    "    0.201, 0.188, 0.17, 0.177, 0.168, 0.171, 0.163, 0.138, 0.12, 0.111,\n",
    "    0.129, 0.116, 0.106, 0.104, 0.111], dtype=np.float32)\n",
    "\n",
    "GPU_CONFIG = [1, 1, 2, 2, 3, 3, 12, 12]\n",
    "TIME_SCALE = 100.0\n",
    "MAX_QUEUE_SIZE = 100\n",
    "\n",
    "# KEY CHANGE: Relaxed deadline multiplier\n",
    "DEADLINE_MIN = 2.0  # Was 1.0\n",
    "DEADLINE_MAX = 4.0  # Was 1.5\n",
    "\n",
    "HOUR_RANGE = 24\n",
    "N_ENVS = 4\n",
    "TOTAL_TIMESTEPS = 200_000\n",
    "\n",
    "print(f\"Deadline slack: {DEADLINE_MIN}-{DEADLINE_MAX}x fastest completion time\")\n",
    "print(f\"(Original was 1.0-1.5x, causing ~90% late jobs)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Queue generation with RELAXED DEADLINES\n",
    "def create_queue_relaxed(hour_range=24, seed=None):\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "    \n",
    "    jobs = []\n",
    "    job_arrival = 0.0\n",
    "    max_time = hour_range * 60.0\n",
    "    \n",
    "    while job_arrival < max_time:\n",
    "        hour_idx = min(int(job_arrival / 60), 24)\n",
    "        rate = INTERARRIVALS[hour_idx] * 20\n",
    "        job_arrival += np.random.exponential(1.0 / rate)\n",
    "        if job_arrival >= max_time:\n",
    "            break\n",
    "        \n",
    "        is_inference = np.random.random() < 0.8\n",
    "        if is_inference:\n",
    "            g1_dur = np.random.exponential(3.0)\n",
    "            if np.random.randint(3) == 2:\n",
    "                g2 = g1_dur/2; g3 = g1_dur/3; g4 = g1_dur/12.5*3.2; g7 = g1_dur/18.4*3.2\n",
    "            else:\n",
    "                g2 = g1_dur/2; g3 = g1_dur/3; g4 = g1_dur/4; g7 = g1_dur/7\n",
    "        else:\n",
    "            g1_dur = np.random.lognormal((np.log(40)+np.log(60))/2, (np.log(60)-np.log(40))/3.29)\n",
    "            if np.random.randint(3) == 2:\n",
    "                g2 = g1_dur/6*3.4; g3 = g1_dur/7.85*3.4; g4 = g1_dur/8.4*3.4; g7 = g1_dur/9.75*3.4\n",
    "            else:\n",
    "                g2 = g1_dur/4.1*2.2; g3 = g1_dur/5.8*2.2; g4 = g1_dur/7.1*2.2; g7 = g1_dur/10.5*2.2\n",
    "        \n",
    "        # RELAXED DEADLINE: 2.0-4.0x instead of 1.0-1.5x\n",
    "        deadline = job_arrival + np.random.uniform(DEADLINE_MIN, DEADLINE_MAX) * g7\n",
    "        jobs.append([job_arrival, deadline, g1_dur, g2, g3, g4, g7])\n",
    "    \n",
    "    return np.array(jobs, dtype=np.float32)\n",
    "\n",
    "q = create_queue_relaxed(24)\n",
    "print(f\"Created {len(q)} jobs\")\n",
    "slack = (q[:, 1] - q[:, 0]) / q[:, 6]  # deadline slack ratio\n",
    "print(f\"Average deadline slack: {np.mean(slack):.2f}x fastest completion time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment (same as before, but uses relaxed queue)\n",
    "class RelaxedSchedulingEnv(gym.Env):\n",
    "    def __init__(self, gpu_config, queue=None, hour_range=24):\n",
    "        super().__init__()\n",
    "        self.gpu_config = gpu_config\n",
    "        self.hour_range = hour_range\n",
    "        self.external_queue = queue\n",
    "        \n",
    "        slices = []\n",
    "        for gpu_id, cfg in enumerate(gpu_config):\n",
    "            for size, _ in MIG_PROFILE[cfg]:\n",
    "                slices.append((gpu_id, len(slices), size))\n",
    "        self.slice_info = np.array(slices, dtype=np.int32)\n",
    "        self.n_slices = len(slices)\n",
    "        self.n_gpus = len(gpu_config)\n",
    "        \n",
    "        self.observation_space = spaces.Dict({\n",
    "            \"next_job\": spaces.Box(-np.inf, np.inf, shape=(4,), dtype=np.float32),\n",
    "            \"queue_stats\": spaces.Box(0, 1, shape=(40,), dtype=np.float32),\n",
    "            \"slices\": spaces.Box(0, 1, shape=(self.n_slices,), dtype=np.float32),\n",
    "            \"extras\": spaces.Box(0, 1, shape=(2,), dtype=np.float32),\n",
    "        })\n",
    "        self.action_space = spaces.Discrete(self.n_slices)\n",
    "        self._obs_next_job = np.zeros(4, dtype=np.float32)\n",
    "        self._obs_queue_stats = np.zeros(40, dtype=np.float32)\n",
    "        self._obs_extras = np.zeros(2, dtype=np.float32)\n",
    "        self._bins = np.array([-100, 0, 0.05, 0.2, 0.5, 1, 5, 10, 20, 30, 1e9], dtype=np.float32)\n",
    "    \n",
    "    def reset(self, *, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        self.jobs = self.external_queue.copy() if self.external_queue is not None else create_queue_relaxed(self.hour_range, seed=seed)\n",
    "        self.n_jobs = len(self.jobs)\n",
    "        self.slice_busy = np.zeros(self.n_slices, dtype=np.int32)\n",
    "        self.slice_job = np.full(self.n_slices, -1, dtype=np.int32)\n",
    "        self.slice_finish = np.zeros(self.n_slices, dtype=np.float32)\n",
    "        self.gpu_energy_time = np.zeros(self.n_gpus, dtype=np.float32)\n",
    "        self.now = 0.0\n",
    "        self.next_arrival_idx = 0\n",
    "        self.working_queue = []\n",
    "        self.completed = np.zeros(self.n_jobs, dtype=bool)\n",
    "        self.total_tardiness = 0.0\n",
    "        self.total_energy = 0.0\n",
    "        self.num_late = 0\n",
    "        if self.n_jobs > 0:\n",
    "            self.now = self.jobs[0, 0]\n",
    "            self.working_queue.append(0)\n",
    "            self.next_arrival_idx = 1\n",
    "        return self._get_obs(), {}\n",
    "    \n",
    "    def _get_obs(self):\n",
    "        if self.working_queue:\n",
    "            self.working_queue.sort(key=lambda j: self.jobs[j, 1])\n",
    "            j = self.working_queue[0]\n",
    "            self._obs_next_job[0] = (self.jobs[j, 1] - self.now) / TIME_SCALE\n",
    "            self._obs_next_job[1] = (self.jobs[j, 2] + self.jobs[j, 3]) / 2 / TIME_SCALE\n",
    "            self._obs_next_job[2] = (self.jobs[j, 4] + self.jobs[j, 5]) / 2 / TIME_SCALE\n",
    "            self._obs_next_job[3] = self.jobs[j, 6] / TIME_SCALE\n",
    "            wq = np.array(self.working_queue)\n",
    "            n = len(wq)\n",
    "            self._obs_queue_stats[0:10] = np.histogram(self.jobs[wq, 1] - self.now, self._bins)[0] / n\n",
    "            self._obs_queue_stats[10:20] = np.histogram((self.jobs[wq, 2] + self.jobs[wq, 3]) / 2, self._bins)[0] / n\n",
    "            self._obs_queue_stats[20:30] = np.histogram((self.jobs[wq, 4] + self.jobs[wq, 5]) / 2, self._bins)[0] / n\n",
    "            self._obs_queue_stats[30:40] = np.histogram(self.jobs[wq, 6], self._bins)[0] / n\n",
    "        else:\n",
    "            self._obs_next_job.fill(0)\n",
    "            self._obs_queue_stats.fill(0)\n",
    "        n_free = np.sum(self.slice_busy == 0)\n",
    "        self._obs_extras[0] = min(len(self.working_queue) / MAX_QUEUE_SIZE, 1.0)\n",
    "        self._obs_extras[1] = n_free / self.n_slices\n",
    "        return {\"next_job\": self._obs_next_job.copy(), \"queue_stats\": self._obs_queue_stats.copy(),\n",
    "                \"slices\": self.slice_busy.astype(np.float32), \"extras\": self._obs_extras.copy()}\n",
    "    \n",
    "    def valid_action_mask(self):\n",
    "        return self.slice_busy == 0\n",
    "    \n",
    "    def _calc_energy(self, gpu_id):\n",
    "        mask = self.slice_info[:, 0] == gpu_id\n",
    "        busy_sizes = self.slice_info[mask & (self.slice_busy == 1), 2]\n",
    "        util = min(int(np.sum(busy_sizes)), 7)\n",
    "        energy = ENERGY_TABLE[util] * (self.now - self.gpu_energy_time[gpu_id])\n",
    "        self.total_energy += energy\n",
    "        self.gpu_energy_time[gpu_id] = self.now\n",
    "    \n",
    "    def step(self, action):\n",
    "        job_idx = self.working_queue.pop(0)\n",
    "        slice_size = self.slice_info[action, 2]\n",
    "        gpu_id = self.slice_info[action, 0]\n",
    "        duration = self.jobs[job_idx, SLICE_DUR_IDX[slice_size]]\n",
    "        self._calc_energy(gpu_id)\n",
    "        self.slice_busy[action] = 1\n",
    "        self.slice_job[action] = job_idx\n",
    "        self.slice_finish[action] = self.now + duration\n",
    "        \n",
    "        if self.working_queue and np.any(self.slice_busy == 0):\n",
    "            return self._get_obs(), 0.0, False, False, {'action_mask': self.valid_action_mask()}\n",
    "        \n",
    "        step_tardiness = 0.0\n",
    "        num_completions = 0\n",
    "        while True:\n",
    "            next_arrival = self.jobs[self.next_arrival_idx, 0] if self.next_arrival_idx < self.n_jobs else 1e12\n",
    "            busy_mask = self.slice_busy == 1\n",
    "            next_completion = np.min(self.slice_finish[busy_mask]) if np.any(busy_mask) else 1e12\n",
    "            if next_arrival >= 1e12 and next_completion >= 1e12:\n",
    "                break\n",
    "            if next_arrival <= next_completion:\n",
    "                self.now = next_arrival\n",
    "                self.working_queue.append(self.next_arrival_idx)\n",
    "                self.next_arrival_idx += 1\n",
    "            else:\n",
    "                self.now = next_completion\n",
    "                completing = np.where((self.slice_finish <= self.now + 1e-9) & busy_mask)[0]\n",
    "                for s in completing:\n",
    "                    j = self.slice_job[s]\n",
    "                    tardiness = max(0.0, self.now - self.jobs[j, 1])\n",
    "                    if tardiness > 0:\n",
    "                        self.total_tardiness += tardiness\n",
    "                        step_tardiness += tardiness\n",
    "                        self.num_late += 1\n",
    "                    self.completed[j] = True\n",
    "                    self.slice_busy[s] = 0\n",
    "                    self.slice_job[s] = -1\n",
    "                    num_completions += 1\n",
    "            for g in range(self.n_gpus):\n",
    "                self._calc_energy(g)\n",
    "            if self.working_queue and np.any(self.slice_busy == 0):\n",
    "                break\n",
    "            if self.next_arrival_idx >= self.n_jobs and not np.any(self.slice_busy == 1) and not self.working_queue:\n",
    "                break\n",
    "        \n",
    "        terminated = np.all(self.completed)\n",
    "        if terminated:\n",
    "            reward = (-self.total_tardiness - 0.0000225 * self.total_energy) / (self.n_jobs * 0.0000225 + 1)\n",
    "            info = {'total_energy': self.total_energy, 'avg_tardiness': self.total_tardiness / self.n_jobs,\n",
    "                    'num_late_jobs': self.num_late, 'total_jobs': self.n_jobs}\n",
    "        else:\n",
    "            reward = (-step_tardiness - 0.0000225 * self.total_energy) / (max(1, num_completions) * 1.0000225)\n",
    "            info = {'total_energy': self.total_energy}\n",
    "        info['action_mask'] = self.valid_action_mask()\n",
    "        return self._get_obs(), reward, terminated, False, info\n",
    "\n",
    "print(\"Environment defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks\n",
    "class ProgressCallback(BaseCallback):\n",
    "    def __init__(self, check_freq=10000):\n",
    "        super().__init__()\n",
    "        self.check_freq = check_freq\n",
    "        self.start_time = None\n",
    "    def _on_training_start(self):\n",
    "        self.start_time = time.time()\n",
    "    def _on_step(self):\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "            elapsed = time.time() - self.start_time\n",
    "            remaining = (self.model._total_timesteps - self.n_calls) / (self.n_calls / elapsed) / 60\n",
    "            print(f\"Step {self.n_calls:,}: {self.n_calls/elapsed:.0f} steps/sec, ~{remaining:.1f} min left\")\n",
    "        return True\n",
    "\n",
    "class LRScheduleCallback(BaseCallback):\n",
    "    def __init__(self, initial_lr=3e-4, final_lr=1e-5):\n",
    "        super().__init__()\n",
    "        self.initial_lr = initial_lr\n",
    "        self.final_lr = final_lr\n",
    "    def _on_step(self):\n",
    "        progress = self.num_timesteps / self.model._total_timesteps\n",
    "        new_lr = self.initial_lr + progress * (self.final_lr - self.initial_lr)\n",
    "        for pg in self.model.policy.optimizer.param_groups:\n",
    "            pg['lr'] = new_lr\n",
    "        return True\n",
    "\n",
    "class EntropyDecayCallback(BaseCallback):\n",
    "    def __init__(self, initial=0.01, final=0.001):\n",
    "        super().__init__()\n",
    "        self.initial = initial\n",
    "        self.final = final\n",
    "    def _on_step(self):\n",
    "        progress = self.num_timesteps / self.model._total_timesteps\n",
    "        self.model.ent_coef = self.initial + progress * (self.final - self.initial)\n",
    "        return True\n",
    "\n",
    "print(\"Callbacks defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training setup\n",
    "def mask_fn(env):\n",
    "    return env.valid_action_mask()\n",
    "\n",
    "def make_env(hour_range=24):\n",
    "    def _init():\n",
    "        return ActionMasker(RelaxedSchedulingEnv(GPU_CONFIG, hour_range=hour_range), mask_fn)\n",
    "    return _init\n",
    "\n",
    "train_env = DummyVecEnv([make_env(HOUR_RANGE) for _ in range(N_ENVS)])\n",
    "print(f\"Created {N_ENVS} envs with {HOUR_RANGE}-hour queues and RELAXED deadlines\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN\n",
    "print(\"=\"*70)\n",
    "print(\"TRAINING WITH RELAXED DEADLINES\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Deadline slack: {DEADLINE_MIN}-{DEADLINE_MAX}x (vs 1.0-1.5x original)\")\n",
    "print(f\"Expected late %: ~20-35% (vs ~85-90% original)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "model = MaskablePPO(\n",
    "    \"MultiInputPolicy\", train_env, verbose=0, device=DEVICE,\n",
    "    n_steps=1024, batch_size=4096, n_epochs=8, learning_rate=3e-4,\n",
    "    gamma=0.99, gae_lambda=0.95, clip_range=0.15, ent_coef=0.01,\n",
    "    vf_coef=0.5, max_grad_norm=0.5,\n",
    "    policy_kwargs=dict(net_arch=[256, 256, 128]),\n",
    ")\n",
    "\n",
    "callbacks = CallbackList([\n",
    "    ProgressCallback(check_freq=20000),\n",
    "    LRScheduleCallback(3e-4, 1e-5),\n",
    "    EntropyDecayCallback(0.01, 0.001),\n",
    "])\n",
    "\n",
    "print(f\"\\nTraining {TOTAL_TIMESTEPS:,} timesteps...\")\n",
    "t0 = time.time()\n",
    "model.learn(total_timesteps=TOTAL_TIMESTEPS, callback=callbacks)\n",
    "print(f\"\\nTraining completed in {(time.time()-t0)/60:.1f} minutes\")\n",
    "model.save(\"relaxed_scheduler_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate\n",
    "def evaluate_all(model, n_episodes=10):\n",
    "    def rl_policy(obs, mask, env): return model.predict(obs, action_masks=mask, deterministic=True)[0]\n",
    "    def random_policy(obs, mask, env): return np.random.choice(np.where(mask)[0])\n",
    "    def largest_first(obs, mask, env): return np.where(mask)[0][np.argmax([env.unwrapped.slice_info[a,2] for a in np.where(mask)[0]])]\n",
    "    def smallest_first(obs, mask, env): return np.where(mask)[0][np.argmin([env.unwrapped.slice_info[a,2] for a in np.where(mask)[0]])]\n",
    "    \n",
    "    methods = {'RL-PPO': rl_policy, 'Largest-First': largest_first, 'Smallest-First': smallest_first, 'Random': random_policy}\n",
    "    results = {n: {'tardiness': [], 'late_frac': [], 'energy': []} for n in methods}\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    seeds = [np.random.randint(0, 100000) for _ in range(n_episodes)]\n",
    "    \n",
    "    for name, policy in methods.items():\n",
    "        print(f\"Evaluating {name}...\")\n",
    "        for seed in seeds:\n",
    "            np.random.seed(seed)\n",
    "            env = ActionMasker(RelaxedSchedulingEnv(GPU_CONFIG, hour_range=24), mask_fn)\n",
    "            obs, _ = env.reset()\n",
    "            done = False\n",
    "            while not done:\n",
    "                mask = get_action_masks(env)\n",
    "                action = policy(obs, mask, env)\n",
    "                obs, _, terminated, truncated, info = env.step(action)\n",
    "                done = terminated or truncated\n",
    "            results[name]['tardiness'].append(info['avg_tardiness'])\n",
    "            results[name]['late_frac'].append(info['num_late_jobs']/info['total_jobs'])\n",
    "            results[name]['energy'].append(info['total_energy'])\n",
    "        print(f\"  Late: {np.mean(results[name]['late_frac'])*100:.1f}%\")\n",
    "    return results\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EVALUATION WITH RELAXED DEADLINES\")\n",
    "print(\"=\"*70)\n",
    "eval_results = evaluate_all(model, n_episodes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results and graphs\n",
    "methods = list(eval_results.keys())\n",
    "colors = ['#2ecc71', '#3498db', '#e74c3c', '#9b59b6']\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Tardiness\n",
    "means = [np.mean(eval_results[m]['tardiness']) for m in methods]\n",
    "stds = [np.std(eval_results[m]['tardiness']) for m in methods]\n",
    "axes[0].bar(range(len(methods)), means, yerr=stds, color=colors, capsize=5)\n",
    "axes[0].set_ylabel('Avg Tardiness')\n",
    "axes[0].set_title('Tardiness (RELAXED Deadlines)')\n",
    "axes[0].set_xticks(range(len(methods)))\n",
    "axes[0].set_xticklabels(methods, rotation=15)\n",
    "\n",
    "# Late %\n",
    "means = [np.mean(eval_results[m]['late_frac'])*100 for m in methods]\n",
    "stds = [np.std(eval_results[m]['late_frac'])*100 for m in methods]\n",
    "axes[1].bar(range(len(methods)), means, yerr=stds, color=colors, capsize=5)\n",
    "axes[1].set_ylabel('Late Jobs (%)')\n",
    "axes[1].set_title('Late % (RELAXED Deadlines)')\n",
    "axes[1].set_xticks(range(len(methods)))\n",
    "axes[1].set_xticklabels(methods, rotation=15)\n",
    "\n",
    "# Energy\n",
    "means = [np.mean(eval_results[m]['energy'])/1e6 for m in methods]\n",
    "stds = [np.std(eval_results[m]['energy'])/1e6 for m in methods]\n",
    "axes[2].bar(range(len(methods)), means, yerr=stds, color=colors, capsize=5)\n",
    "axes[2].set_ylabel('Energy (MJ)')\n",
    "axes[2].set_title('Energy Consumption')\n",
    "axes[2].set_xticks(range(len(methods)))\n",
    "axes[2].set_xticklabels(methods, rotation=15)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('relaxed_results.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "# Summary\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"RESULTS SUMMARY (RELAXED DEADLINES)\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'Method':<20} {'Late %':>10} {'Tardiness':>12} {'Energy (MJ)':>12}\")\n",
    "print(\"-\"*60)\n",
    "for m in methods:\n",
    "    late = np.mean(eval_results[m]['late_frac'])*100\n",
    "    tard = np.mean(eval_results[m]['tardiness'])\n",
    "    energy = np.mean(eval_results[m]['energy'])/1e6\n",
    "    print(f\"{m:<20} {late:>9.1f}% {tard:>12.3f} {energy:>12.2f}\")\n",
    "\n",
    "# Improvement\n",
    "rl_late = np.mean(eval_results['RL-PPO']['late_frac'])\n",
    "best_baseline = min([m for m in methods if m != 'RL-PPO'], key=lambda m: np.mean(eval_results[m]['late_frac']))\n",
    "baseline_late = np.mean(eval_results[best_baseline]['late_frac'])\n",
    "improvement = (baseline_late - rl_late) / baseline_late * 100\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"RL-PPO improvement over {best_baseline}: {improvement:+.1f}% reduction in late jobs\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

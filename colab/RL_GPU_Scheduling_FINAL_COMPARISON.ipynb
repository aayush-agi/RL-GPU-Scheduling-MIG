{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL GPU Scheduling - FINAL COMPARISON\n",
    "## Comparing Original Paper, Original Colab, and Our Improvements\n",
    "\n",
    "---\n",
    "\n",
    "## Summary of Approaches\n",
    "\n",
    "### 1. Original Paper/Colab Configuration\n",
    "- **Environment**: Pandas-based (slow)\n",
    "- **Deadlines**: 1.0-1.5× fastest completion (very tight)\n",
    "- **Network**: [256, 256]\n",
    "- **Training**: 200k timesteps\n",
    "- **Baselines**: None implemented in colab\n",
    "- **Result**: ~85-90% late jobs (too tight deadlines make problem trivial)\n",
    "\n",
    "### 2. Our Optimizations Tried\n",
    "\n",
    "| Version | Change | Result | Issue |\n",
    "|---------|--------|--------|-------|\n",
    "| **Fast** | NumPy env (10-50× faster) | Same accuracy, faster | Training/eval mismatch |\n",
    "| **Improved** | +LR annealing, +entropy decay, deeper net | ~48% late | Still worse than heuristics |\n",
    "| **Relaxed** | Deadlines 2-4× | ~48% late | RL still losing |\n",
    "| **Enhanced** | +Slice sizes in obs, +immediate rewards | Pending | Better signal for learning |\n",
    "\n",
    "### 3. Key Finding\n",
    "\n",
    "**The original problem formulation is greedy-optimal:**\n",
    "- Larger slice → faster completion → less tardiness\n",
    "- \"Largest-First\" heuristic achieves near-optimal results\n",
    "- RL struggles to discover this simple rule from scratch\n",
    "\n",
    "---\n",
    "\n",
    "## This Notebook\n",
    "\n",
    "Runs **fair comparison** of:\n",
    "1. **Original Config** - Paper's hyperparameters, tight deadlines\n",
    "2. **Improved Config** - Our enhancements, tight deadlines\n",
    "3. **Relaxed Config** - Our enhancements, relaxed deadlines\n",
    "4. **Heuristic Baselines** - EFT, Largest-First, Smallest-First, Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Install\n",
    "%pip install -q stable-baselines3 sb3-contrib gymnasium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Imports\n",
    "import numpy as np\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.callbacks import BaseCallback, CallbackList\n",
    "from sb3_contrib.ppo_mask import MaskablePPO\n",
    "from sb3_contrib.common.wrappers import ActionMasker\n",
    "from sb3_contrib.common.maskable.utils import get_action_masks\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Device: {DEVICE}')\n",
    "if DEVICE == 'cuda':\n",
    "    print(f'GPU: {torch.cuda.get_device_name(0)}')\n",
    "else:\n",
    "    print('⚠️  WARNING: Using CPU - training will be slow!')\n",
    "    print('   Enable GPU runtime: Runtime → Change runtime type → GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Constants (same as original paper)\n",
    "MIG_PROFILE = {\n",
    "    1: [(7, 40)], 2: [(4, 20), (3, 20)], 3: [(4, 20), (2, 10), (1, 10)],\n",
    "    4: [(4, 20), (1, 5), (1, 5), (1, 5)], 5: [(3, 20), (3, 20)],\n",
    "    6: [(3, 20), (2, 10), (1, 10)], 7: [(3, 20), (1, 10), (1, 5), (1, 5)],\n",
    "    8: [(2, 10), (2, 10), (3, 20)], 9: [(2, 10), (1, 5), (1, 5), (3, 20)],\n",
    "    10: [(1, 5), (1, 5), (2, 10), (3, 20)], 11: [(1, 5), (1, 5), (1, 5), (1, 5), (3, 20)],\n",
    "    12: [(2, 10), (2, 10), (2, 10), (1, 10)], 13: [(2, 10), (1, 5), (1, 5), (2, 10), (1, 10)],\n",
    "    14: [(1, 5), (1, 5), (2, 10), (2, 10), (1, 10)], 15: [(2, 10), (1, 10), (1, 5), (1, 5), (1, 5), (1, 5)],\n",
    "    16: [(1, 5), (1, 5), (2, 10), (1, 10), (1, 5), (1, 5)],\n",
    "    17: [(1, 5), (1, 5), (1, 10), (1, 5), (2, 10), (1, 5)],\n",
    "    18: [(1, 5), (1, 5), (1, 10), (1, 5), (1, 5), (2, 10)],\n",
    "    19: [(1, 5), (1, 5), (1, 5), (1, 5), (1, 5), (1, 5), (1, 5)]\n",
    "}\n",
    "\n",
    "ENERGY_TABLE = np.array([40, 120, 160, 200, 240, 250, 250, 250], dtype=np.float32)\n",
    "SLICE_DUR_IDX = {1: 2, 2: 3, 3: 4, 4: 5, 7: 6}\n",
    "INTERARRIVALS = np.array([0.111, 0.083, 0.085, 0.1, 0.137, 0.169, 0.171, 0.169, 0.179, 0.191,\n",
    "    0.201, 0.188, 0.17, 0.177, 0.168, 0.171, 0.163, 0.138, 0.12, 0.111,\n",
    "    0.129, 0.116, 0.106, 0.104, 0.111], dtype=np.float32)\n",
    "\n",
    "GPU_CONFIG = [1, 1, 2, 2, 3, 3, 12, 12]\n",
    "TIME_SCALE = 100.0\n",
    "MAX_QUEUE_SIZE = 100\n",
    "\n",
    "print(\"Constants loaded (matching original paper)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Queue generation - supports both tight and relaxed deadlines\n",
    "def create_queue(hour_range=24, deadline_min=1.0, deadline_max=1.5, seed=None):\n",
    "    \"\"\"\n",
    "    Create job queue.\n",
    "    \n",
    "    Args:\n",
    "        deadline_min, deadline_max: Multipliers for fastest completion time\n",
    "            Original paper: 1.0-1.5 (very tight)\n",
    "            Relaxed: 2.0-4.0 (learnable)\n",
    "    \"\"\"\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "    \n",
    "    jobs = []\n",
    "    arrival = 0.0\n",
    "    max_time = hour_range * 60.0\n",
    "    \n",
    "    while arrival < max_time:\n",
    "        hour_idx = min(int(arrival / 60), 24)\n",
    "        rate = INTERARRIVALS[hour_idx] * 20\n",
    "        arrival += np.random.exponential(1.0 / rate)\n",
    "        if arrival >= max_time:\n",
    "            break\n",
    "        \n",
    "        is_inference = np.random.random() < 0.8\n",
    "        if is_inference:\n",
    "            g1 = np.random.exponential(3.0)\n",
    "            if np.random.randint(3) == 2:\n",
    "                g2, g3, g4, g7 = g1/2, g1/3, g1/12.5*3.2, g1/18.4*3.2\n",
    "            else:\n",
    "                g2, g3, g4, g7 = g1/2, g1/3, g1/4, g1/7\n",
    "        else:\n",
    "            g1 = np.random.lognormal((np.log(40)+np.log(60))/2, (np.log(60)-np.log(40))/3.29)\n",
    "            if np.random.randint(3) == 2:\n",
    "                g2, g3, g4, g7 = g1/6*3.4, g1/7.85*3.4, g1/8.4*3.4, g1/9.75*3.4\n",
    "            else:\n",
    "                g2, g3, g4, g7 = g1/4.1*2.2, g1/5.8*2.2, g1/7.1*2.2, g1/10.5*2.2\n",
    "        \n",
    "        deadline = arrival + np.random.uniform(deadline_min, deadline_max) * g7\n",
    "        jobs.append([arrival, deadline, g1, g2, g3, g4, g7])\n",
    "    \n",
    "    return np.array(jobs, dtype=np.float32)\n",
    "\n",
    "# Test both\n",
    "q_tight = create_queue(24, 1.0, 1.5, seed=42)\n",
    "q_relaxed = create_queue(24, 2.0, 4.0, seed=42)\n",
    "slack_tight = np.mean((q_tight[:, 1] - q_tight[:, 0]) / q_tight[:, 6])\n",
    "slack_relaxed = np.mean((q_relaxed[:, 1] - q_relaxed[:, 0]) / q_relaxed[:, 6])\n",
    "\n",
    "print(f\"Queue sizes: {len(q_tight)} jobs\")\n",
    "print(f\"Tight deadlines (original): {slack_tight:.2f}× avg slack\")\n",
    "print(f\"Relaxed deadlines (ours):   {slack_relaxed:.2f}× avg slack\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Environment (NumPy-optimized, supports both configurations)\n",
    "class SchedulingEnv(gym.Env):\n",
    "    \"\"\"NumPy-optimized scheduling environment.\"\"\"\n",
    "    \n",
    "    def __init__(self, gpu_config, queue=None, hour_range=24, \n",
    "                 deadline_min=1.0, deadline_max=1.5, enhanced_obs=False):\n",
    "        super().__init__()\n",
    "        self.gpu_config = gpu_config\n",
    "        self.hour_range = hour_range\n",
    "        self.external_queue = queue\n",
    "        self.deadline_min = deadline_min\n",
    "        self.deadline_max = deadline_max\n",
    "        self.enhanced_obs = enhanced_obs\n",
    "        \n",
    "        slices = []\n",
    "        for gpu_id, cfg in enumerate(gpu_config):\n",
    "            for size, _ in MIG_PROFILE[cfg]:\n",
    "                slices.append((gpu_id, len(slices), size))\n",
    "        self.slice_info = np.array(slices, dtype=np.int32)\n",
    "        self.n_slices = len(slices)\n",
    "        self.n_gpus = len(gpu_config)\n",
    "        self.slice_sizes_norm = self.slice_info[:, 2].astype(np.float32) / 7.0\n",
    "        \n",
    "        # Observation space\n",
    "        if enhanced_obs:\n",
    "            self.observation_space = spaces.Dict({\n",
    "                \"next_job\": spaces.Box(-np.inf, np.inf, shape=(5,), dtype=np.float32),\n",
    "                \"queue_stats\": spaces.Box(0, 1, shape=(40,), dtype=np.float32),\n",
    "                \"slice_busy\": spaces.Box(0, 1, shape=(self.n_slices,), dtype=np.float32),\n",
    "                \"slice_sizes\": spaces.Box(0, 1, shape=(self.n_slices,), dtype=np.float32),\n",
    "                \"extras\": spaces.Box(0, 1, shape=(3,), dtype=np.float32),\n",
    "            })\n",
    "            self._obs_next_job = np.zeros(5, dtype=np.float32)\n",
    "            self._obs_extras = np.zeros(3, dtype=np.float32)\n",
    "        else:\n",
    "            self.observation_space = spaces.Dict({\n",
    "                \"next_job\": spaces.Box(-np.inf, np.inf, shape=(4,), dtype=np.float32),\n",
    "                \"queue_stats\": spaces.Box(0, 1, shape=(40,), dtype=np.float32),\n",
    "                \"slices\": spaces.Box(0, 1, shape=(self.n_slices,), dtype=np.float32),\n",
    "                \"extras\": spaces.Box(0, 1, shape=(2,), dtype=np.float32),\n",
    "            })\n",
    "            self._obs_next_job = np.zeros(4, dtype=np.float32)\n",
    "            self._obs_extras = np.zeros(2, dtype=np.float32)\n",
    "        \n",
    "        self.action_space = spaces.Discrete(self.n_slices)\n",
    "        self._obs_queue_stats = np.zeros(40, dtype=np.float32)\n",
    "        self._bins = np.array([-100, 0, 0.05, 0.2, 0.5, 1, 5, 10, 20, 30, 1e9], dtype=np.float32)\n",
    "    \n",
    "    def reset(self, *, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        if self.external_queue is not None:\n",
    "            self.jobs = self.external_queue.copy()\n",
    "        else:\n",
    "            self.jobs = create_queue(self.hour_range, self.deadline_min, self.deadline_max, seed)\n",
    "        self.n_jobs = len(self.jobs)\n",
    "        self.slice_busy = np.zeros(self.n_slices, dtype=np.int32)\n",
    "        self.slice_job = np.full(self.n_slices, -1, dtype=np.int32)\n",
    "        self.slice_finish = np.zeros(self.n_slices, dtype=np.float32)\n",
    "        self.gpu_energy_time = np.zeros(self.n_gpus, dtype=np.float32)\n",
    "        self.now = 0.0\n",
    "        self.next_arrival_idx = 0\n",
    "        self.working_queue = []\n",
    "        self.completed = np.zeros(self.n_jobs, dtype=bool)\n",
    "        self.total_tardiness = 0.0\n",
    "        self.total_energy = 0.0\n",
    "        self.num_late = 0\n",
    "        \n",
    "        if self.n_jobs > 0:\n",
    "            self.now = self.jobs[0, 0]\n",
    "            self.working_queue.append(0)\n",
    "            self.next_arrival_idx = 1\n",
    "        return self._get_obs(), {}\n",
    "    \n",
    "    def _get_obs(self):\n",
    "        if self.working_queue:\n",
    "            self.working_queue.sort(key=lambda j: self.jobs[j, 1])\n",
    "            j = self.working_queue[0]\n",
    "            time_to_deadline = self.jobs[j, 1] - self.now\n",
    "            self._obs_next_job[0] = time_to_deadline / TIME_SCALE\n",
    "            self._obs_next_job[1] = (self.jobs[j, 2] + self.jobs[j, 3]) / 2 / TIME_SCALE\n",
    "            self._obs_next_job[2] = (self.jobs[j, 4] + self.jobs[j, 5]) / 2 / TIME_SCALE\n",
    "            self._obs_next_job[3] = self.jobs[j, 6] / TIME_SCALE\n",
    "            if self.enhanced_obs:\n",
    "                self._obs_next_job[4] = min(1.0, self.jobs[j, 6] / max(time_to_deadline, 0.01))\n",
    "            \n",
    "            wq = np.array(self.working_queue)\n",
    "            n = len(wq)\n",
    "            self._obs_queue_stats[0:10] = np.histogram(self.jobs[wq, 1] - self.now, self._bins)[0] / n\n",
    "            self._obs_queue_stats[10:20] = np.histogram((self.jobs[wq, 2] + self.jobs[wq, 3]) / 2, self._bins)[0] / n\n",
    "            self._obs_queue_stats[20:30] = np.histogram((self.jobs[wq, 4] + self.jobs[wq, 5]) / 2, self._bins)[0] / n\n",
    "            self._obs_queue_stats[30:40] = np.histogram(self.jobs[wq, 6], self._bins)[0] / n\n",
    "        else:\n",
    "            self._obs_next_job.fill(0)\n",
    "            self._obs_queue_stats.fill(0)\n",
    "        \n",
    "        free_mask = self.slice_busy == 0\n",
    "        n_free = np.sum(free_mask)\n",
    "        \n",
    "        if self.enhanced_obs:\n",
    "            self._obs_extras[0] = min(len(self.working_queue) / MAX_QUEUE_SIZE, 1.0)\n",
    "            self._obs_extras[1] = n_free / self.n_slices\n",
    "            self._obs_extras[2] = np.max(self.slice_info[free_mask, 2]) / 7.0 if n_free > 0 else 0.0\n",
    "            return {\n",
    "                \"next_job\": self._obs_next_job.copy(),\n",
    "                \"queue_stats\": self._obs_queue_stats.copy(),\n",
    "                \"slice_busy\": self.slice_busy.astype(np.float32),\n",
    "                \"slice_sizes\": self.slice_sizes_norm.copy(),\n",
    "                \"extras\": self._obs_extras.copy(),\n",
    "            }\n",
    "        else:\n",
    "            self._obs_extras[0] = min(len(self.working_queue) / MAX_QUEUE_SIZE, 1.0)\n",
    "            self._obs_extras[1] = n_free / self.n_slices\n",
    "            return {\n",
    "                \"next_job\": self._obs_next_job.copy(),\n",
    "                \"queue_stats\": self._obs_queue_stats.copy(),\n",
    "                \"slices\": self.slice_busy.astype(np.float32),\n",
    "                \"extras\": self._obs_extras.copy(),\n",
    "            }\n",
    "    \n",
    "    def valid_action_mask(self):\n",
    "        return self.slice_busy == 0\n",
    "    \n",
    "    def _calc_energy(self, gpu_id):\n",
    "        mask = self.slice_info[:, 0] == gpu_id\n",
    "        busy_sizes = self.slice_info[mask & (self.slice_busy == 1), 2]\n",
    "        util = min(int(np.sum(busy_sizes)), 7)\n",
    "        energy = ENERGY_TABLE[util] * (self.now - self.gpu_energy_time[gpu_id])\n",
    "        self.total_energy += energy\n",
    "        self.gpu_energy_time[gpu_id] = self.now\n",
    "    \n",
    "    def step(self, action):\n",
    "        job_idx = self.working_queue.pop(0)\n",
    "        slice_size = self.slice_info[action, 2]\n",
    "        gpu_id = self.slice_info[action, 0]\n",
    "        duration = self.jobs[job_idx, SLICE_DUR_IDX[slice_size]]\n",
    "        \n",
    "        self._calc_energy(gpu_id)\n",
    "        self.slice_busy[action] = 1\n",
    "        self.slice_job[action] = job_idx\n",
    "        self.slice_finish[action] = self.now + duration\n",
    "        \n",
    "        if self.working_queue and np.any(self.slice_busy == 0):\n",
    "            return self._get_obs(), 0.0, False, False, {'action_mask': self.valid_action_mask()}\n",
    "        \n",
    "        step_tardiness = 0.0\n",
    "        num_completions = 0\n",
    "        \n",
    "        while True:\n",
    "            next_arrival = self.jobs[self.next_arrival_idx, 0] if self.next_arrival_idx < self.n_jobs else 1e12\n",
    "            busy_mask = self.slice_busy == 1\n",
    "            next_completion = np.min(self.slice_finish[busy_mask]) if np.any(busy_mask) else 1e12\n",
    "            \n",
    "            if next_arrival >= 1e12 and next_completion >= 1e12:\n",
    "                break\n",
    "            \n",
    "            if next_arrival <= next_completion:\n",
    "                self.now = next_arrival\n",
    "                self.working_queue.append(self.next_arrival_idx)\n",
    "                self.next_arrival_idx += 1\n",
    "            else:\n",
    "                self.now = next_completion\n",
    "                completing = np.where((self.slice_finish <= self.now + 1e-9) & busy_mask)[0]\n",
    "                for s in completing:\n",
    "                    j = self.slice_job[s]\n",
    "                    tardiness = max(0.0, self.now - self.jobs[j, 1])\n",
    "                    if tardiness > 0:\n",
    "                        self.total_tardiness += tardiness\n",
    "                        step_tardiness += tardiness\n",
    "                        self.num_late += 1\n",
    "                    self.completed[j] = True\n",
    "                    self.slice_busy[s] = 0\n",
    "                    self.slice_job[s] = -1\n",
    "                    num_completions += 1\n",
    "            \n",
    "            for g in range(self.n_gpus):\n",
    "                self._calc_energy(g)\n",
    "            \n",
    "            if self.working_queue and np.any(self.slice_busy == 0):\n",
    "                break\n",
    "            if self.next_arrival_idx >= self.n_jobs and not np.any(self.slice_busy == 1) and not self.working_queue:\n",
    "                break\n",
    "        \n",
    "        terminated = np.all(self.completed)\n",
    "        \n",
    "        if terminated:\n",
    "            reward = (-self.total_tardiness - 0.0000225 * self.total_energy) / (self.n_jobs * 0.0000225 + 1)\n",
    "            info = {\n",
    "                'total_energy': self.total_energy,\n",
    "                'avg_tardiness': self.total_tardiness / self.n_jobs,\n",
    "                'num_late_jobs': self.num_late,\n",
    "                'total_jobs': self.n_jobs,\n",
    "            }\n",
    "        else:\n",
    "            reward = (-step_tardiness - 0.0000225 * self.total_energy) / (max(1, num_completions) * 1.0000225)\n",
    "            info = {'total_energy': self.total_energy}\n",
    "        \n",
    "        info['action_mask'] = self.valid_action_mask()\n",
    "        return self._get_obs(), reward, terminated, False, info\n",
    "\n",
    "print(\"Environment ready (supports both original and improved configs)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Callbacks\n",
    "class ProgressCallback(BaseCallback):\n",
    "    def __init__(self, check_freq=10000):\n",
    "        super().__init__()\n",
    "        self.check_freq = check_freq\n",
    "        self.start_time = None\n",
    "    def _on_training_start(self):\n",
    "        self.start_time = time.time()\n",
    "    def _on_step(self):\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "            elapsed = time.time() - self.start_time\n",
    "            sps = self.n_calls / elapsed\n",
    "            remaining = (self.model._total_timesteps - self.n_calls) / sps / 60\n",
    "            print(f\"  Step {self.n_calls:,}: {sps:.0f} sps, ~{remaining:.1f} min left\")\n",
    "        return True\n",
    "\n",
    "class LRScheduleCallback(BaseCallback):\n",
    "    def __init__(self, initial=3e-4, final=1e-5):\n",
    "        super().__init__()\n",
    "        self.initial = initial\n",
    "        self.final = final\n",
    "    def _on_step(self):\n",
    "        progress = self.num_timesteps / self.model._total_timesteps\n",
    "        lr = self.initial + progress * (self.final - self.initial)\n",
    "        for pg in self.model.policy.optimizer.param_groups:\n",
    "            pg['lr'] = lr\n",
    "        return True\n",
    "\n",
    "class EntropyDecayCallback(BaseCallback):\n",
    "    def __init__(self, initial=0.01, final=0.001):\n",
    "        super().__init__()\n",
    "        self.initial = initial\n",
    "        self.final = final\n",
    "    def _on_step(self):\n",
    "        progress = self.num_timesteps / self.model._total_timesteps\n",
    "        self.model.ent_coef = self.initial + progress * (self.final - self.initial)\n",
    "        return True\n",
    "\n",
    "print(\"Callbacks ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Training function\n",
    "def mask_fn(env):\n",
    "    return env.valid_action_mask()\n",
    "\n",
    "def train_model(config_name, deadline_min, deadline_max, enhanced_obs, \n",
    "                net_arch, batch_size, n_epochs, timesteps, use_annealing):\n",
    "    \"\"\"Train a model with specified configuration.\"\"\"\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"TRAINING: {config_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"  Deadlines: {deadline_min}-{deadline_max}×\")\n",
    "    print(f\"  Network: {net_arch}\")\n",
    "    print(f\"  Batch: {batch_size}, Epochs: {n_epochs}\")\n",
    "    print(f\"  Timesteps: {timesteps:,}\")\n",
    "    print(f\"  LR/Entropy annealing: {use_annealing}\")\n",
    "    print(f\"  Enhanced obs: {enhanced_obs}\")\n",
    "    \n",
    "    def make_env():\n",
    "        def _init():\n",
    "            env = SchedulingEnv(GPU_CONFIG, hour_range=24, \n",
    "                               deadline_min=deadline_min, deadline_max=deadline_max,\n",
    "                               enhanced_obs=enhanced_obs)\n",
    "            return ActionMasker(env, mask_fn)\n",
    "        return _init\n",
    "    \n",
    "    train_env = DummyVecEnv([make_env() for _ in range(4)])\n",
    "    \n",
    "    model = MaskablePPO(\n",
    "        \"MultiInputPolicy\",\n",
    "        train_env,\n",
    "        verbose=0,\n",
    "        device=DEVICE,\n",
    "        n_steps=1024,\n",
    "        batch_size=batch_size,\n",
    "        n_epochs=n_epochs,\n",
    "        learning_rate=3e-4,\n",
    "        gamma=0.99,\n",
    "        gae_lambda=0.95,\n",
    "        clip_range=0.2 if not use_annealing else 0.15,\n",
    "        ent_coef=0.001 if not use_annealing else 0.01,\n",
    "        vf_coef=0.5,\n",
    "        max_grad_norm=0.5,\n",
    "        policy_kwargs=dict(net_arch=net_arch),\n",
    "    )\n",
    "    \n",
    "    if use_annealing:\n",
    "        callbacks = CallbackList([\n",
    "            ProgressCallback(check_freq=20000),\n",
    "            LRScheduleCallback(3e-4, 1e-5),\n",
    "            EntropyDecayCallback(0.01, 0.001),\n",
    "        ])\n",
    "    else:\n",
    "        callbacks = ProgressCallback(check_freq=20000)\n",
    "    \n",
    "    t0 = time.time()\n",
    "    model.learn(total_timesteps=timesteps, callback=callbacks)\n",
    "    elapsed = time.time() - t0\n",
    "    print(f\"  ✓ Completed in {elapsed/60:.1f} min\")\n",
    "    \n",
    "    train_env.close()\n",
    "    return model\n",
    "\n",
    "print(\"Training function ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Evaluation function\n",
    "def evaluate_model(model, config_name, deadline_min, deadline_max, enhanced_obs, n_episodes=10):\n",
    "    \"\"\"Evaluate model and return results.\"\"\"\n",
    "    \n",
    "    results = {'tardiness': [], 'late_frac': [], 'energy': []}\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    seeds = [np.random.randint(0, 100000) for _ in range(n_episodes)]\n",
    "    \n",
    "    for seed in seeds:\n",
    "        np.random.seed(seed)\n",
    "        env = ActionMasker(SchedulingEnv(GPU_CONFIG, hour_range=24,\n",
    "                                         deadline_min=deadline_min, deadline_max=deadline_max,\n",
    "                                         enhanced_obs=enhanced_obs), mask_fn)\n",
    "        obs, _ = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            mask = get_action_masks(env)\n",
    "            action, _ = model.predict(obs, action_masks=mask, deterministic=True)\n",
    "            obs, _, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "        results['tardiness'].append(info['avg_tardiness'])\n",
    "        results['late_frac'].append(info['num_late_jobs'] / info['total_jobs'])\n",
    "        results['energy'].append(info['total_energy'])\n",
    "    \n",
    "    return results\n",
    "\n",
    "def evaluate_heuristics(deadline_min, deadline_max, n_episodes=10):\n",
    "    \"\"\"Evaluate heuristic baselines.\"\"\"\n",
    "    \n",
    "    def largest_first(mask, env):\n",
    "        valid = np.where(mask)[0]\n",
    "        sizes = [env.unwrapped.slice_info[a, 2] for a in valid]\n",
    "        return valid[np.argmax(sizes)]\n",
    "    \n",
    "    def smallest_first(mask, env):\n",
    "        valid = np.where(mask)[0]\n",
    "        sizes = [env.unwrapped.slice_info[a, 2] for a in valid]\n",
    "        return valid[np.argmin(sizes)]\n",
    "    \n",
    "    def random_policy(mask, env):\n",
    "        return np.random.choice(np.where(mask)[0])\n",
    "    \n",
    "    def eft_policy(mask, env, obs):\n",
    "        valid = np.where(mask)[0]\n",
    "        best_action = valid[0]\n",
    "        best_finish = float('inf')\n",
    "        for a in valid:\n",
    "            size = env.unwrapped.slice_info[a, 2]\n",
    "            dur = obs['next_job'][3] * TIME_SCALE * (7.0 / size)  # Approximate\n",
    "            if dur < best_finish:\n",
    "                best_finish = dur\n",
    "                best_action = a\n",
    "        return best_action\n",
    "    \n",
    "    heuristics = {\n",
    "        'Largest-First': lambda m, e, o: largest_first(m, e),\n",
    "        'Smallest-First': lambda m, e, o: smallest_first(m, e),\n",
    "        'EFT': eft_policy,\n",
    "        'Random': lambda m, e, o: random_policy(m, e),\n",
    "    }\n",
    "    \n",
    "    all_results = {}\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    seeds = [np.random.randint(0, 100000) for _ in range(n_episodes)]\n",
    "    \n",
    "    for name, policy in heuristics.items():\n",
    "        results = {'tardiness': [], 'late_frac': [], 'energy': []}\n",
    "        for seed in seeds:\n",
    "            np.random.seed(seed)\n",
    "            env = ActionMasker(SchedulingEnv(GPU_CONFIG, hour_range=24,\n",
    "                                             deadline_min=deadline_min, deadline_max=deadline_max), mask_fn)\n",
    "            obs, _ = env.reset()\n",
    "            done = False\n",
    "            while not done:\n",
    "                mask = get_action_masks(env)\n",
    "                action = policy(mask, env, obs)\n",
    "                obs, _, terminated, truncated, info = env.step(action)\n",
    "                done = terminated or truncated\n",
    "            results['tardiness'].append(info['avg_tardiness'])\n",
    "            results['late_frac'].append(info['num_late_jobs'] / info['total_jobs'])\n",
    "            results['energy'].append(info['total_energy'])\n",
    "        all_results[name] = results\n",
    "    \n",
    "    return all_results\n",
    "\n",
    "print(\"Evaluation functions ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: TRAIN ALL CONFIGURATIONS\n",
    "print(\"=\"*70)\n",
    "print(\"TRAINING ALL CONFIGURATIONS\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nThis will train 3 models:\")\n",
    "print(\"  1. Original Paper Config (tight deadlines, basic hyperparams)\")\n",
    "print(\"  2. Improved Config (tight deadlines, our improvements)\")\n",
    "print(\"  3. Relaxed Config (relaxed deadlines, our improvements)\")\n",
    "print(\"\\nEstimated time: ~45-90 min total on A100\")\n",
    "\n",
    "# 1. Original Paper Config\n",
    "model_original = train_model(\n",
    "    config_name=\"Original Paper\",\n",
    "    deadline_min=1.0, deadline_max=1.5,  # Tight (original)\n",
    "    enhanced_obs=False,\n",
    "    net_arch=[256, 256],  # Original\n",
    "    batch_size=2048,      # Original\n",
    "    n_epochs=5,           # Original\n",
    "    timesteps=200_000,    # Original\n",
    "    use_annealing=False,  # Original\n",
    ")\n",
    "\n",
    "# 2. Improved Config (tight deadlines)\n",
    "model_improved = train_model(\n",
    "    config_name=\"Improved (Tight)\",\n",
    "    deadline_min=1.0, deadline_max=1.5,  # Tight (same as original)\n",
    "    enhanced_obs=False,\n",
    "    net_arch=[256, 256, 128],  # Deeper\n",
    "    batch_size=4096,           # Larger\n",
    "    n_epochs=8,                # More\n",
    "    timesteps=200_000,         # Same\n",
    "    use_annealing=True,        # NEW\n",
    ")\n",
    "\n",
    "# 3. Relaxed Config\n",
    "model_relaxed = train_model(\n",
    "    config_name=\"Improved (Relaxed)\",\n",
    "    deadline_min=2.0, deadline_max=4.0,  # RELAXED\n",
    "    enhanced_obs=True,                    # Enhanced\n",
    "    net_arch=[256, 256, 128],\n",
    "    batch_size=4096,\n",
    "    n_epochs=8,\n",
    "    timesteps=200_000,\n",
    "    use_annealing=True,\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ALL MODELS TRAINED!\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: EVALUATE ALL\n",
    "print(\"=\"*70)\n",
    "print(\"EVALUATING ALL CONFIGURATIONS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "N_EVAL = 10\n",
    "\n",
    "# Evaluate RL models\n",
    "print(\"\\nEvaluating RL models...\")\n",
    "results_original = evaluate_model(model_original, \"Original\", 1.0, 1.5, False, N_EVAL)\n",
    "print(f\"  Original Paper: {np.mean(results_original['late_frac'])*100:.1f}% late\")\n",
    "\n",
    "results_improved = evaluate_model(model_improved, \"Improved\", 1.0, 1.5, False, N_EVAL)\n",
    "print(f\"  Improved (Tight): {np.mean(results_improved['late_frac'])*100:.1f}% late\")\n",
    "\n",
    "results_relaxed = evaluate_model(model_relaxed, \"Relaxed\", 2.0, 4.0, True, N_EVAL)\n",
    "print(f\"  Improved (Relaxed): {np.mean(results_relaxed['late_frac'])*100:.1f}% late\")\n",
    "\n",
    "# Evaluate heuristics on TIGHT deadlines\n",
    "print(\"\\nEvaluating heuristics (tight deadlines)...\")\n",
    "heuristics_tight = evaluate_heuristics(1.0, 1.5, N_EVAL)\n",
    "for name, res in heuristics_tight.items():\n",
    "    print(f\"  {name}: {np.mean(res['late_frac'])*100:.1f}% late\")\n",
    "\n",
    "# Evaluate heuristics on RELAXED deadlines\n",
    "print(\"\\nEvaluating heuristics (relaxed deadlines)...\")\n",
    "heuristics_relaxed = evaluate_heuristics(2.0, 4.0, N_EVAL)\n",
    "for name, res in heuristics_relaxed.items():\n",
    "    print(f\"  {name}: {np.mean(res['late_frac'])*100:.1f}% late\")\n",
    "\n",
    "print(\"\\n✓ Evaluation complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11: COMPREHENSIVE RESULTS TABLE\n",
    "print(\"\\n\" + \"=\"*90)\n",
    "print(\"COMPREHENSIVE RESULTS COMPARISON\")\n",
    "print(\"=\"*90)\n",
    "\n",
    "def fmt_result(results):\n",
    "    late = np.mean(results['late_frac']) * 100\n",
    "    late_std = np.std(results['late_frac']) * 100\n",
    "    tard = np.mean(results['tardiness'])\n",
    "    tard_std = np.std(results['tardiness'])\n",
    "    energy = np.mean(results['energy']) / 1e6\n",
    "    return f\"{late:5.1f}±{late_std:4.1f}%  {tard:6.2f}±{tard_std:5.2f}  {energy:5.2f}\"\n",
    "\n",
    "print(f\"\\n{'Method':<35} {'Late %':>12} {'Tardiness':>14} {'Energy(MJ)':>10}\")\n",
    "print(\"-\"*85)\n",
    "\n",
    "print(\"\\n--- TIGHT DEADLINES (Original Paper Config: 1.0-1.5×) ---\")\n",
    "print(f\"{'RL: Original Paper Config':<35} {fmt_result(results_original)}\")\n",
    "print(f\"{'RL: Our Improvements (Tight)':<35} {fmt_result(results_improved)}\")\n",
    "for name, res in heuristics_tight.items():\n",
    "    print(f\"{name:<35} {fmt_result(res)}\")\n",
    "\n",
    "print(\"\\n--- RELAXED DEADLINES (Our Config: 2.0-4.0×) ---\")\n",
    "print(f\"{'RL: Our Improvements (Relaxed)':<35} {fmt_result(results_relaxed)}\")\n",
    "for name, res in heuristics_relaxed.items():\n",
    "    print(f\"{name:<35} {fmt_result(res)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12: ANALYSIS\n",
    "print(\"\\n\" + \"=\"*90)\n",
    "print(\"ANALYSIS\")\n",
    "print(\"=\"*90)\n",
    "\n",
    "# Tight deadlines analysis\n",
    "rl_orig_late = np.mean(results_original['late_frac'])\n",
    "rl_impr_late = np.mean(results_improved['late_frac'])\n",
    "best_heur_tight = min(heuristics_tight.items(), key=lambda x: np.mean(x[1]['late_frac']))\n",
    "best_heur_tight_late = np.mean(best_heur_tight[1]['late_frac'])\n",
    "\n",
    "print(\"\\n1. TIGHT DEADLINES (Original Paper Setup)\")\n",
    "print(f\"   Best heuristic: {best_heur_tight[0]} ({best_heur_tight_late*100:.1f}% late)\")\n",
    "print(f\"   RL Original:    {rl_orig_late*100:.1f}% late\")\n",
    "print(f\"   RL Improved:    {rl_impr_late*100:.1f}% late\")\n",
    "\n",
    "if rl_impr_late < best_heur_tight_late:\n",
    "    imp = (best_heur_tight_late - rl_impr_late) / best_heur_tight_late * 100\n",
    "    print(f\"   → RL WINS by {imp:.1f}%\")\n",
    "else:\n",
    "    gap = (rl_impr_late - best_heur_tight_late) / best_heur_tight_late * 100\n",
    "    print(f\"   → RL loses by {gap:.1f}% (tight deadlines are greedy-optimal)\")\n",
    "\n",
    "# Relaxed deadlines analysis\n",
    "rl_relax_late = np.mean(results_relaxed['late_frac'])\n",
    "best_heur_relax = min(heuristics_relaxed.items(), key=lambda x: np.mean(x[1]['late_frac']))\n",
    "best_heur_relax_late = np.mean(best_heur_relax[1]['late_frac'])\n",
    "\n",
    "print(\"\\n2. RELAXED DEADLINES (Our Setup)\")\n",
    "print(f\"   Best heuristic: {best_heur_relax[0]} ({best_heur_relax_late*100:.1f}% late)\")\n",
    "print(f\"   RL Improved:    {rl_relax_late*100:.1f}% late\")\n",
    "\n",
    "if rl_relax_late < best_heur_relax_late:\n",
    "    imp = (best_heur_relax_late - rl_relax_late) / best_heur_relax_late * 100\n",
    "    print(f\"   → RL WINS by {imp:.1f}%\")\n",
    "else:\n",
    "    gap = (rl_relax_late - best_heur_relax_late) / best_heur_relax_late * 100\n",
    "    print(f\"   → RL loses by {gap:.1f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13: VISUALIZATION\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Tight Deadlines\n",
    "ax = axes[0]\n",
    "methods_tight = ['RL (Original)', 'RL (Improved)'] + list(heuristics_tight.keys())\n",
    "lates_tight = [np.mean(results_original['late_frac'])*100, \n",
    "               np.mean(results_improved['late_frac'])*100] + \\\n",
    "              [np.mean(heuristics_tight[m]['late_frac'])*100 for m in heuristics_tight]\n",
    "colors = ['#e74c3c', '#3498db'] + ['#95a5a6']*len(heuristics_tight)\n",
    "bars = ax.bar(range(len(methods_tight)), lates_tight, color=colors, edgecolor='black')\n",
    "ax.set_ylabel('Late Jobs (%)', fontsize=12, fontweight='bold')\n",
    "ax.set_title('TIGHT Deadlines (1.0-1.5×)\\n(Original Paper Config)', fontsize=12, fontweight='bold')\n",
    "ax.set_xticks(range(len(methods_tight)))\n",
    "ax.set_xticklabels(methods_tight, rotation=30, ha='right', fontsize=9)\n",
    "for bar, val in zip(bars, lates_tight):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1, f'{val:.1f}%', \n",
    "            ha='center', fontsize=9, fontweight='bold')\n",
    "\n",
    "# Plot 2: Relaxed Deadlines\n",
    "ax = axes[1]\n",
    "methods_relax = ['RL (Relaxed)'] + list(heuristics_relaxed.keys())\n",
    "lates_relax = [np.mean(results_relaxed['late_frac'])*100] + \\\n",
    "              [np.mean(heuristics_relaxed[m]['late_frac'])*100 for m in heuristics_relaxed]\n",
    "colors = ['#27ae60'] + ['#95a5a6']*len(heuristics_relaxed)\n",
    "bars = ax.bar(range(len(methods_relax)), lates_relax, color=colors, edgecolor='black')\n",
    "ax.set_ylabel('Late Jobs (%)', fontsize=12, fontweight='bold')\n",
    "ax.set_title('RELAXED Deadlines (2.0-4.0×)\\n(Our Improved Config)', fontsize=12, fontweight='bold')\n",
    "ax.set_xticks(range(len(methods_relax)))\n",
    "ax.set_xticklabels(methods_relax, rotation=30, ha='right', fontsize=9)\n",
    "for bar, val in zip(bars, lates_relax):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1, f'{val:.1f}%', \n",
    "            ha='center', fontsize=9, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('final_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"\\n✓ Figure saved as 'final_comparison.png'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 14: LATEX OUTPUT\n",
    "print(\"\\n\" + \"=\"*90)\n",
    "print(\"LATEX TABLE (Copy for paper)\")\n",
    "print(\"=\"*90)\n",
    "\n",
    "latex = r\"\"\"\n",
    "\\begin{table}[htbp]\n",
    "\\centering\n",
    "\\caption{Comparison of GPU Scheduling Methods}\n",
    "\\label{tab:comparison}\n",
    "\\begin{tabular}{llccc}\n",
    "\\toprule\n",
    "\\textbf{Deadlines} & \\textbf{Method} & \\textbf{Late (\\%)} & \\textbf{Tardiness} & \\textbf{Energy (MJ)} \\\\\n",
    "\\midrule\n",
    "\\multirow{6}{*}{Tight (1.0-1.5$\\times$)} \n",
    "\"\"\"\n",
    "\n",
    "# Tight deadlines\n",
    "def add_row(name, results, bold=False):\n",
    "    late = np.mean(results['late_frac']) * 100\n",
    "    late_std = np.std(results['late_frac']) * 100\n",
    "    tard = np.mean(results['tardiness'])\n",
    "    tard_std = np.std(results['tardiness'])\n",
    "    energy = np.mean(results['energy']) / 1e6\n",
    "    \n",
    "    if bold:\n",
    "        return f\"& \\\\textbf{{{name}}} & \\\\textbf{{{late:.1f}$\\\\pm${late_std:.1f}}} & \\\\textbf{{{tard:.2f}$\\\\pm${tard_std:.2f}}} & \\\\textbf{{{energy:.2f}}} \\\\\\\\\\n\"\n",
    "    else:\n",
    "        return f\"& {name} & {late:.1f}$\\\\pm${late_std:.1f} & {tard:.2f}$\\\\pm${tard_std:.2f} & {energy:.2f} \\\\\\\\\\n\"\n",
    "\n",
    "latex += add_row(\"RL (Original Paper)\", results_original, True)\n",
    "latex += add_row(\"RL (Our Improvements)\", results_improved, True)\n",
    "for name, res in heuristics_tight.items():\n",
    "    latex += add_row(name, res)\n",
    "\n",
    "latex += r\"\"\"\\midrule\n",
    "\\multirow{5}{*}{Relaxed (2.0-4.0$\\times$)}\n",
    "\"\"\"\n",
    "\n",
    "latex += add_row(\"RL (Our Improvements)\", results_relaxed, True)\n",
    "for name, res in heuristics_relaxed.items():\n",
    "    latex += add_row(name, res)\n",
    "\n",
    "latex += r\"\"\"\\bottomrule\n",
    "\\end{tabular}\n",
    "\\end{table}\n",
    "\"\"\"\n",
    "\n",
    "print(latex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary of Findings\n",
    "\n",
    "## Approaches Tried\n",
    "\n",
    "| Version | Key Changes | Result |\n",
    "|---------|-------------|--------|\n",
    "| **Original Paper** | Pandas env, tight deadlines, [256,256], 200k steps | ~85-90% late |\n",
    "| **Fast** | NumPy env (10-50× faster) | Same accuracy |\n",
    "| **Improved (Tight)** | +LR annealing, +entropy decay, [256,256,128] | ~48% late |\n",
    "| **Improved (Relaxed)** | Deadlines 2-4×, enhanced obs | ~40-50% late |\n",
    "\n",
    "## Key Finding\n",
    "\n",
    "**The original problem is greedy-optimal:**\n",
    "- Larger GPU slice → faster job completion → less tardiness\n",
    "- Simple \"Largest-First\" heuristic achieves near-optimal\n",
    "- RL struggles because there's no complex trade-off to learn\n",
    "\n",
    "## Recommendations\n",
    "\n",
    "1. **For tight deadlines**: Use Largest-First heuristic (simpler, faster, effective)\n",
    "2. **For RL to win**: Need problem with multi-objective trade-offs or constraints\n",
    "3. **Our speed improvement**: 10-50× faster training with NumPy environment"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL GPU Scheduling - ENHANCED VERSION\n",
    "## With Better Observation Space and Reward Shaping\n",
    "\n",
    "---\n",
    "\n",
    "### Key Enhancements Over Previous Version\n",
    "\n",
    "| Enhancement | Previous | This Version | Why |\n",
    "|-------------|----------|--------------|-----|\n",
    "| **Slice Info** | Just busy/free | + Slice sizes | RL knows which slice is larger |\n",
    "| **Reward** | End-of-episode | + Per-step early bonus | Faster feedback |\n",
    "| **Training** | 200k steps | 500k steps | More learning |\n",
    "| **Exploration** | ent=0.01→0.001 | ent=0.02→0.001 | More exploration early |\n",
    "\n",
    "### The Core Problem\n",
    "\n",
    "RL was losing to Largest-First because:\n",
    "1. The observation didn't tell RL which slice was largest\n",
    "2. No immediate reward for picking fast-completing slices\n",
    "3. Not enough training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q stable-baselines3 sb3-contrib gymnasium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.callbacks import BaseCallback, CallbackList\n",
    "from sb3_contrib.ppo_mask import MaskablePPO\n",
    "from sb3_contrib.common.wrappers import ActionMasker\n",
    "from sb3_contrib.common.maskable.utils import get_action_masks\n",
    "\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f'Device: {DEVICE}')\n",
    "if DEVICE == 'cuda':\n",
    "    print(f'GPU: {torch.cuda.get_device_name(0)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "MIG_PROFILE = {\n",
    "    1: [(7, 40)], 2: [(4, 20), (3, 20)], 3: [(4, 20), (2, 10), (1, 10)],\n",
    "    4: [(4, 20), (1, 5), (1, 5), (1, 5)], 5: [(3, 20), (3, 20)],\n",
    "    6: [(3, 20), (2, 10), (1, 10)], 7: [(3, 20), (1, 10), (1, 5), (1, 5)],\n",
    "    8: [(2, 10), (2, 10), (3, 20)], 9: [(2, 10), (1, 5), (1, 5), (3, 20)],\n",
    "    10: [(1, 5), (1, 5), (2, 10), (3, 20)], 11: [(1, 5), (1, 5), (1, 5), (1, 5), (3, 20)],\n",
    "    12: [(2, 10), (2, 10), (2, 10), (1, 10)], 13: [(2, 10), (1, 5), (1, 5), (2, 10), (1, 10)],\n",
    "    14: [(1, 5), (1, 5), (2, 10), (2, 10), (1, 10)], 15: [(2, 10), (1, 10), (1, 5), (1, 5), (1, 5), (1, 5)],\n",
    "    16: [(1, 5), (1, 5), (2, 10), (1, 10), (1, 5), (1, 5)],\n",
    "    17: [(1, 5), (1, 5), (1, 10), (1, 5), (2, 10), (1, 5)],\n",
    "    18: [(1, 5), (1, 5), (1, 10), (1, 5), (1, 5), (2, 10)],\n",
    "    19: [(1, 5), (1, 5), (1, 5), (1, 5), (1, 5), (1, 5), (1, 5)]\n",
    "}\n",
    "\n",
    "ENERGY_TABLE = np.array([40, 120, 160, 200, 240, 250, 250, 250], dtype=np.float32)\n",
    "SLICE_DUR_IDX = {1: 2, 2: 3, 3: 4, 4: 5, 7: 6}\n",
    "INTERARRIVALS = np.array([0.111, 0.083, 0.085, 0.1, 0.137, 0.169, 0.171, 0.169, 0.179, 0.191,\n",
    "    0.201, 0.188, 0.17, 0.177, 0.168, 0.171, 0.163, 0.138, 0.12, 0.111,\n",
    "    0.129, 0.116, 0.106, 0.104, 0.111], dtype=np.float32)\n",
    "\n",
    "GPU_CONFIG = [1, 1, 2, 2, 3, 3, 12, 12]\n",
    "TIME_SCALE = 100.0\n",
    "MAX_QUEUE_SIZE = 100\n",
    "\n",
    "DEADLINE_MIN = 2.0\n",
    "DEADLINE_MAX = 4.0\n",
    "\n",
    "# ENHANCED: More training\n",
    "HOUR_RANGE = 24\n",
    "N_ENVS = 8  # More parallel envs\n",
    "TOTAL_TIMESTEPS = 500_000  # 2.5x more training\n",
    "\n",
    "print(f\"Enhanced Configuration:\")\n",
    "print(f\"  Training: {TOTAL_TIMESTEPS:,} timesteps (was 200k)\")\n",
    "print(f\"  Parallel envs: {N_ENVS} (was 4)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_queue(hour_range=24, seed=None):\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "    jobs = []\n",
    "    arrival = 0.0\n",
    "    max_time = hour_range * 60.0\n",
    "    while arrival < max_time:\n",
    "        hour_idx = min(int(arrival / 60), 24)\n",
    "        rate = INTERARRIVALS[hour_idx] * 20\n",
    "        arrival += np.random.exponential(1.0 / rate)\n",
    "        if arrival >= max_time:\n",
    "            break\n",
    "        is_inference = np.random.random() < 0.8\n",
    "        if is_inference:\n",
    "            g1 = np.random.exponential(3.0)\n",
    "            if np.random.randint(3) == 2:\n",
    "                g2, g3, g4, g7 = g1/2, g1/3, g1/12.5*3.2, g1/18.4*3.2\n",
    "            else:\n",
    "                g2, g3, g4, g7 = g1/2, g1/3, g1/4, g1/7\n",
    "        else:\n",
    "            g1 = np.random.lognormal((np.log(40)+np.log(60))/2, (np.log(60)-np.log(40))/3.29)\n",
    "            if np.random.randint(3) == 2:\n",
    "                g2, g3, g4, g7 = g1/6*3.4, g1/7.85*3.4, g1/8.4*3.4, g1/9.75*3.4\n",
    "            else:\n",
    "                g2, g3, g4, g7 = g1/4.1*2.2, g1/5.8*2.2, g1/7.1*2.2, g1/10.5*2.2\n",
    "        deadline = arrival + np.random.uniform(DEADLINE_MIN, DEADLINE_MAX) * g7\n",
    "        jobs.append([arrival, deadline, g1, g2, g3, g4, g7])\n",
    "    return np.array(jobs, dtype=np.float32)\n",
    "\n",
    "print(f\"Queue generator ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENHANCED Environment with slice sizes in observation\n",
    "class EnhancedSchedulingEnv(gym.Env):\n",
    "    \"\"\"Enhanced environment with better observation space.\"\"\"\n",
    "    \n",
    "    def __init__(self, gpu_config, queue=None, hour_range=24):\n",
    "        super().__init__()\n",
    "        self.gpu_config = gpu_config\n",
    "        self.hour_range = hour_range\n",
    "        self.external_queue = queue\n",
    "        \n",
    "        slices = []\n",
    "        for gpu_id, cfg in enumerate(gpu_config):\n",
    "            for size, _ in MIG_PROFILE[cfg]:\n",
    "                slices.append((gpu_id, len(slices), size))\n",
    "        self.slice_info = np.array(slices, dtype=np.int32)\n",
    "        self.n_slices = len(slices)\n",
    "        self.n_gpus = len(gpu_config)\n",
    "        \n",
    "        # ENHANCED: Include slice sizes normalized [0,1]\n",
    "        self.slice_sizes_norm = self.slice_info[:, 2].astype(np.float32) / 7.0\n",
    "        \n",
    "        # ENHANCED: Richer observation space\n",
    "        self.observation_space = spaces.Dict({\n",
    "            \"next_job\": spaces.Box(-np.inf, np.inf, shape=(5,), dtype=np.float32),  # +urgency\n",
    "            \"queue_stats\": spaces.Box(0, 1, shape=(40,), dtype=np.float32),\n",
    "            \"slice_busy\": spaces.Box(0, 1, shape=(self.n_slices,), dtype=np.float32),\n",
    "            \"slice_sizes\": spaces.Box(0, 1, shape=(self.n_slices,), dtype=np.float32),  # NEW!\n",
    "            \"extras\": spaces.Box(0, 1, shape=(3,), dtype=np.float32),  # +max_available_size\n",
    "        })\n",
    "        self.action_space = spaces.Discrete(self.n_slices)\n",
    "        \n",
    "        self._obs_next_job = np.zeros(5, dtype=np.float32)\n",
    "        self._obs_queue_stats = np.zeros(40, dtype=np.float32)\n",
    "        self._obs_extras = np.zeros(3, dtype=np.float32)\n",
    "        self._bins = np.array([-100, 0, 0.05, 0.2, 0.5, 1, 5, 10, 20, 30, 1e9], dtype=np.float32)\n",
    "    \n",
    "    def reset(self, *, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        self.jobs = self.external_queue.copy() if self.external_queue is not None else create_queue(self.hour_range, seed)\n",
    "        self.n_jobs = len(self.jobs)\n",
    "        self.slice_busy = np.zeros(self.n_slices, dtype=np.int32)\n",
    "        self.slice_job = np.full(self.n_slices, -1, dtype=np.int32)\n",
    "        self.slice_finish = np.zeros(self.n_slices, dtype=np.float32)\n",
    "        self.gpu_energy_time = np.zeros(self.n_gpus, dtype=np.float32)\n",
    "        self.now = 0.0\n",
    "        self.next_arrival_idx = 0\n",
    "        self.working_queue = []\n",
    "        self.completed = np.zeros(self.n_jobs, dtype=bool)\n",
    "        self.total_tardiness = 0.0\n",
    "        self.total_energy = 0.0\n",
    "        self.num_late = 0\n",
    "        self.num_early = 0  # Track early completions\n",
    "        \n",
    "        if self.n_jobs > 0:\n",
    "            self.now = self.jobs[0, 0]\n",
    "            self.working_queue.append(0)\n",
    "            self.next_arrival_idx = 1\n",
    "        return self._get_obs(), {}\n",
    "    \n",
    "    def _get_obs(self):\n",
    "        if self.working_queue:\n",
    "            self.working_queue.sort(key=lambda j: self.jobs[j, 1])\n",
    "            j = self.working_queue[0]\n",
    "            time_to_deadline = self.jobs[j, 1] - self.now\n",
    "            fastest_completion = self.jobs[j, 6]  # g7 duration\n",
    "            \n",
    "            self._obs_next_job[0] = time_to_deadline / TIME_SCALE\n",
    "            self._obs_next_job[1] = (self.jobs[j, 2] + self.jobs[j, 3]) / 2 / TIME_SCALE\n",
    "            self._obs_next_job[2] = (self.jobs[j, 4] + self.jobs[j, 5]) / 2 / TIME_SCALE\n",
    "            self._obs_next_job[3] = fastest_completion / TIME_SCALE\n",
    "            # ENHANCED: Urgency ratio (how tight is the deadline?)\n",
    "            self._obs_next_job[4] = min(1.0, fastest_completion / max(time_to_deadline, 0.01))\n",
    "            \n",
    "            wq = np.array(self.working_queue)\n",
    "            n = len(wq)\n",
    "            self._obs_queue_stats[0:10] = np.histogram(self.jobs[wq, 1] - self.now, self._bins)[0] / n\n",
    "            self._obs_queue_stats[10:20] = np.histogram((self.jobs[wq, 2] + self.jobs[wq, 3]) / 2, self._bins)[0] / n\n",
    "            self._obs_queue_stats[20:30] = np.histogram((self.jobs[wq, 4] + self.jobs[wq, 5]) / 2, self._bins)[0] / n\n",
    "            self._obs_queue_stats[30:40] = np.histogram(self.jobs[wq, 6], self._bins)[0] / n\n",
    "        else:\n",
    "            self._obs_next_job.fill(0)\n",
    "            self._obs_queue_stats.fill(0)\n",
    "        \n",
    "        free_mask = self.slice_busy == 0\n",
    "        n_free = np.sum(free_mask)\n",
    "        self._obs_extras[0] = min(len(self.working_queue) / MAX_QUEUE_SIZE, 1.0)\n",
    "        self._obs_extras[1] = n_free / self.n_slices\n",
    "        # ENHANCED: Max available slice size\n",
    "        if n_free > 0:\n",
    "            max_avail_size = np.max(self.slice_info[free_mask, 2])\n",
    "            self._obs_extras[2] = max_avail_size / 7.0\n",
    "        else:\n",
    "            self._obs_extras[2] = 0.0\n",
    "        \n",
    "        return {\n",
    "            \"next_job\": self._obs_next_job.copy(),\n",
    "            \"queue_stats\": self._obs_queue_stats.copy(),\n",
    "            \"slice_busy\": self.slice_busy.astype(np.float32),\n",
    "            \"slice_sizes\": self.slice_sizes_norm.copy(),  # NEW!\n",
    "            \"extras\": self._obs_extras.copy(),\n",
    "        }\n",
    "    \n",
    "    def valid_action_mask(self):\n",
    "        return self.slice_busy == 0\n",
    "    \n",
    "    def _calc_energy(self, gpu_id):\n",
    "        mask = self.slice_info[:, 0] == gpu_id\n",
    "        busy_sizes = self.slice_info[mask & (self.slice_busy == 1), 2]\n",
    "        util = min(int(np.sum(busy_sizes)), 7)\n",
    "        energy = ENERGY_TABLE[util] * (self.now - self.gpu_energy_time[gpu_id])\n",
    "        self.total_energy += energy\n",
    "        self.gpu_energy_time[gpu_id] = self.now\n",
    "    \n",
    "    def step(self, action):\n",
    "        job_idx = self.working_queue.pop(0)\n",
    "        slice_size = self.slice_info[action, 2]\n",
    "        gpu_id = self.slice_info[action, 0]\n",
    "        duration = self.jobs[job_idx, SLICE_DUR_IDX[slice_size]]\n",
    "        deadline = self.jobs[job_idx, 1]\n",
    "        \n",
    "        self._calc_energy(gpu_id)\n",
    "        self.slice_busy[action] = 1\n",
    "        self.slice_job[action] = job_idx\n",
    "        self.slice_finish[action] = self.now + duration\n",
    "        \n",
    "        # ENHANCED: Immediate reward for slice choice\n",
    "        # Reward for choosing larger slices (faster completion)\n",
    "        immediate_reward = 0.01 * (slice_size / 7.0)  # Small bonus for larger slice\n",
    "        \n",
    "        # Check if this choice will meet deadline\n",
    "        expected_finish = self.now + duration\n",
    "        if expected_finish <= deadline:\n",
    "            immediate_reward += 0.05  # Bonus for expected on-time completion\n",
    "        \n",
    "        if self.working_queue and np.any(self.slice_busy == 0):\n",
    "            return self._get_obs(), immediate_reward, False, False, {'action_mask': self.valid_action_mask()}\n",
    "        \n",
    "        step_tardiness = 0.0\n",
    "        step_early = 0\n",
    "        num_completions = 0\n",
    "        \n",
    "        while True:\n",
    "            next_arrival = self.jobs[self.next_arrival_idx, 0] if self.next_arrival_idx < self.n_jobs else 1e12\n",
    "            busy_mask = self.slice_busy == 1\n",
    "            next_completion = np.min(self.slice_finish[busy_mask]) if np.any(busy_mask) else 1e12\n",
    "            \n",
    "            if next_arrival >= 1e12 and next_completion >= 1e12:\n",
    "                break\n",
    "            \n",
    "            if next_arrival <= next_completion:\n",
    "                self.now = next_arrival\n",
    "                self.working_queue.append(self.next_arrival_idx)\n",
    "                self.next_arrival_idx += 1\n",
    "            else:\n",
    "                self.now = next_completion\n",
    "                completing = np.where((self.slice_finish <= self.now + 1e-9) & busy_mask)[0]\n",
    "                for s in completing:\n",
    "                    j = self.slice_job[s]\n",
    "                    tardiness = max(0.0, self.now - self.jobs[j, 1])\n",
    "                    if tardiness > 0:\n",
    "                        self.total_tardiness += tardiness\n",
    "                        step_tardiness += tardiness\n",
    "                        self.num_late += 1\n",
    "                    else:\n",
    "                        self.num_early += 1\n",
    "                        step_early += 1\n",
    "                    self.completed[j] = True\n",
    "                    self.slice_busy[s] = 0\n",
    "                    self.slice_job[s] = -1\n",
    "                    num_completions += 1\n",
    "            \n",
    "            for g in range(self.n_gpus):\n",
    "                self._calc_energy(g)\n",
    "            \n",
    "            if self.working_queue and np.any(self.slice_busy == 0):\n",
    "                break\n",
    "            if self.next_arrival_idx >= self.n_jobs and not np.any(self.slice_busy == 1) and not self.working_queue:\n",
    "                break\n",
    "        \n",
    "        terminated = np.all(self.completed)\n",
    "        \n",
    "        # ENHANCED: Better reward shaping\n",
    "        if terminated:\n",
    "            # Final reward based on overall performance\n",
    "            late_fraction = self.num_late / self.n_jobs\n",
    "            avg_tard = self.total_tardiness / self.n_jobs\n",
    "            # Reward for low late fraction and low tardiness\n",
    "            reward = (1.0 - late_fraction) * 10.0 - avg_tard - 0.0000225 * self.total_energy / self.n_jobs\n",
    "            info = {\n",
    "                'total_energy': self.total_energy,\n",
    "                'avg_tardiness': avg_tard,\n",
    "                'num_late_jobs': self.num_late,\n",
    "                'total_jobs': self.n_jobs,\n",
    "            }\n",
    "        else:\n",
    "            # ENHANCED: Reward for each early completion, penalty for late\n",
    "            reward = immediate_reward + 0.1 * step_early - 0.2 * step_tardiness\n",
    "            info = {'total_energy': self.total_energy}\n",
    "        \n",
    "        info['action_mask'] = self.valid_action_mask()\n",
    "        return self._get_obs(), reward, terminated, False, info\n",
    "\n",
    "print(\"Enhanced environment ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Callbacks\n",
    "class ProgressCallback(BaseCallback):\n",
    "    def __init__(self, check_freq=10000):\n",
    "        super().__init__()\n",
    "        self.check_freq = check_freq\n",
    "        self.start_time = None\n",
    "    def _on_training_start(self):\n",
    "        self.start_time = time.time()\n",
    "    def _on_step(self):\n",
    "        if self.n_calls % self.check_freq == 0:\n",
    "            elapsed = time.time() - self.start_time\n",
    "            sps = self.n_calls / elapsed\n",
    "            remaining = (self.model._total_timesteps - self.n_calls) / sps / 60\n",
    "            print(f\"Step {self.n_calls:,}: {sps:.0f} sps, ~{remaining:.1f} min left\")\n",
    "        return True\n",
    "\n",
    "class LRScheduleCallback(BaseCallback):\n",
    "    def __init__(self, initial=3e-4, final=1e-5):\n",
    "        super().__init__()\n",
    "        self.initial = initial\n",
    "        self.final = final\n",
    "    def _on_step(self):\n",
    "        progress = self.num_timesteps / self.model._total_timesteps\n",
    "        lr = self.initial + progress * (self.final - self.initial)\n",
    "        for pg in self.model.policy.optimizer.param_groups:\n",
    "            pg['lr'] = lr\n",
    "        return True\n",
    "\n",
    "class EntropyDecayCallback(BaseCallback):\n",
    "    def __init__(self, initial=0.02, final=0.001):  # ENHANCED: Higher initial\n",
    "        super().__init__()\n",
    "        self.initial = initial\n",
    "        self.final = final\n",
    "    def _on_step(self):\n",
    "        progress = self.num_timesteps / self.model._total_timesteps\n",
    "        self.model.ent_coef = self.initial + progress * (self.final - self.initial)\n",
    "        return True\n",
    "\n",
    "print(\"Callbacks ready!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "def mask_fn(env):\n",
    "    return env.valid_action_mask()\n",
    "\n",
    "def make_env(hour_range=24):\n",
    "    def _init():\n",
    "        return ActionMasker(EnhancedSchedulingEnv(GPU_CONFIG, hour_range=hour_range), mask_fn)\n",
    "    return _init\n",
    "\n",
    "train_env = DummyVecEnv([make_env(HOUR_RANGE) for _ in range(N_ENVS)])\n",
    "print(f\"Created {N_ENVS} parallel environments\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN\n",
    "print(\"=\"*70)\n",
    "print(\"ENHANCED TRAINING\")\n",
    "print(\"=\"*70)\n",
    "print(\"Enhancements:\")\n",
    "print(\"  - Slice sizes in observation (RL knows which is largest)\")\n",
    "print(\"  - Urgency ratio for next job\")\n",
    "print(\"  - Max available slice size in extras\")\n",
    "print(\"  - Immediate rewards for good slice choices\")\n",
    "print(\"  - 500k timesteps (was 200k)\")\n",
    "print(\"  - Higher initial entropy (0.02 vs 0.01)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "model = MaskablePPO(\n",
    "    \"MultiInputPolicy\",\n",
    "    train_env,\n",
    "    verbose=0,\n",
    "    device=DEVICE,\n",
    "    n_steps=1024,\n",
    "    batch_size=4096,\n",
    "    n_epochs=10,      # ENHANCED: more epochs\n",
    "    learning_rate=3e-4,\n",
    "    gamma=0.99,\n",
    "    gae_lambda=0.95,\n",
    "    clip_range=0.15,\n",
    "    ent_coef=0.02,    # ENHANCED: higher initial\n",
    "    vf_coef=0.5,\n",
    "    max_grad_norm=0.5,\n",
    "    policy_kwargs=dict(net_arch=[256, 256, 128]),\n",
    ")\n",
    "\n",
    "callbacks = CallbackList([\n",
    "    ProgressCallback(check_freq=50000),\n",
    "    LRScheduleCallback(3e-4, 1e-5),\n",
    "    EntropyDecayCallback(0.02, 0.001),\n",
    "])\n",
    "\n",
    "print(f\"\\nTraining {TOTAL_TIMESTEPS:,} timesteps on {DEVICE}...\")\n",
    "print(\"(This will take ~60-90 min on A100)\")\n",
    "t0 = time.time()\n",
    "model.learn(total_timesteps=TOTAL_TIMESTEPS, callback=callbacks)\n",
    "elapsed = time.time() - t0\n",
    "print(f\"\\n✓ Training completed in {elapsed/60:.1f} minutes\")\n",
    "model.save(\"enhanced_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "def evaluate_all(model, n_episodes=10, hour_range=24):\n",
    "    def rl_policy(obs, mask, env):\n",
    "        return model.predict(obs, action_masks=mask, deterministic=True)[0]\n",
    "    def random_policy(obs, mask, env):\n",
    "        return np.random.choice(np.where(mask)[0])\n",
    "    def largest_first(obs, mask, env):\n",
    "        valid = np.where(mask)[0]\n",
    "        sizes = [env.unwrapped.slice_info[a, 2] for a in valid]\n",
    "        return valid[np.argmax(sizes)]\n",
    "    def smallest_first(obs, mask, env):\n",
    "        valid = np.where(mask)[0]\n",
    "        sizes = [env.unwrapped.slice_info[a, 2] for a in valid]\n",
    "        return valid[np.argmin(sizes)]\n",
    "    def eft_policy(obs, mask, env):\n",
    "        valid = np.where(mask)[0]\n",
    "        next_job = obs['next_job']\n",
    "        dur_small = next_job[1] * TIME_SCALE\n",
    "        dur_med = next_job[2] * TIME_SCALE\n",
    "        dur_large = next_job[3] * TIME_SCALE\n",
    "        best_action = valid[0]\n",
    "        best_finish = float('inf')\n",
    "        for a in valid:\n",
    "            size = env.unwrapped.slice_info[a, 2]\n",
    "            if size in (1, 2): dur = dur_small\n",
    "            elif size in (3, 4): dur = dur_med\n",
    "            else: dur = dur_large\n",
    "            if dur < best_finish:\n",
    "                best_finish = dur\n",
    "                best_action = a\n",
    "        return best_action\n",
    "    \n",
    "    methods = {'RL-PPO (Enhanced)': rl_policy, 'EFT': eft_policy, 'Largest-First': largest_first, \n",
    "               'Smallest-First': smallest_first, 'Random': random_policy}\n",
    "    results = {n: {'tardiness': [], 'late_frac': [], 'energy': []} for n in methods}\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    seeds = [np.random.randint(0, 100000) for _ in range(n_episodes)]\n",
    "    \n",
    "    print(f\"Evaluating on {n_episodes} episodes...\\n\")\n",
    "    for name, policy in methods.items():\n",
    "        print(f\"{name}...\", end=\" \")\n",
    "        for seed in seeds:\n",
    "            np.random.seed(seed)\n",
    "            env = ActionMasker(EnhancedSchedulingEnv(GPU_CONFIG, hour_range=hour_range), mask_fn)\n",
    "            obs, _ = env.reset()\n",
    "            done = False\n",
    "            while not done:\n",
    "                mask = get_action_masks(env)\n",
    "                action = policy(obs, mask, env)\n",
    "                obs, _, terminated, truncated, info = env.step(action)\n",
    "                done = terminated or truncated\n",
    "            results[name]['tardiness'].append(info['avg_tardiness'])\n",
    "            results[name]['late_frac'].append(info['num_late_jobs'] / info['total_jobs'])\n",
    "            results[name]['energy'].append(info['total_energy'])\n",
    "        print(f\"Late: {np.mean(results[name]['late_frac'])*100:.1f}%\")\n",
    "    return results\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"EVALUATION\")\n",
    "print(\"=\"*70)\n",
    "results = evaluate_all(model, n_episodes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graphs\n",
    "methods = list(results.keys())\n",
    "colors = ['#27ae60', '#3498db', '#9b59b6', '#e74c3c', '#95a5a6']\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "# Late %\n",
    "ax = axes[0]\n",
    "means = [np.mean(results[m]['late_frac']) * 100 for m in methods]\n",
    "stds = [np.std(results[m]['late_frac']) * 100 for m in methods]\n",
    "bars = ax.bar(range(len(methods)), means, yerr=stds, color=colors, capsize=5, edgecolor='black')\n",
    "ax.set_ylabel('Late Jobs (%)', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Late Job Percentage', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(range(len(methods)))\n",
    "ax.set_xticklabels([m.replace(' ', '\\n').replace('(', '\\n(') for m in methods], fontsize=8)\n",
    "for bar, val in zip(bars, means):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 2, f'{val:.1f}%', ha='center', fontsize=9, fontweight='bold')\n",
    "\n",
    "# Tardiness\n",
    "ax = axes[1]\n",
    "means = [np.mean(results[m]['tardiness']) for m in methods]\n",
    "stds = [np.std(results[m]['tardiness']) for m in methods]\n",
    "bars = ax.bar(range(len(methods)), means, yerr=stds, color=colors, capsize=5, edgecolor='black')\n",
    "ax.set_ylabel('Avg Tardiness', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Average Tardiness', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(range(len(methods)))\n",
    "ax.set_xticklabels([m.replace(' ', '\\n').replace('(', '\\n(') for m in methods], fontsize=8)\n",
    "\n",
    "# Energy\n",
    "ax = axes[2]\n",
    "means = [np.mean(results[m]['energy']) / 1e6 for m in methods]\n",
    "stds = [np.std(results[m]['energy']) / 1e6 for m in methods]\n",
    "bars = ax.bar(range(len(methods)), means, yerr=stds, color=colors, capsize=5, edgecolor='black')\n",
    "ax.set_ylabel('Energy (MJ)', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Energy Consumption', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(range(len(methods)))\n",
    "ax.set_xticklabels([m.replace(' ', '\\n').replace('(', '\\n(') for m in methods], fontsize=8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('enhanced_results.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary\n",
    "print(\"=\"*80)\n",
    "print(\"ENHANCED RESULTS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\n{'Method':<25} {'Late %':>12} {'Tardiness':>14} {'Energy (MJ)':>14}\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "for method in methods:\n",
    "    late = np.mean(results[method]['late_frac']) * 100\n",
    "    late_std = np.std(results[method]['late_frac']) * 100\n",
    "    tard = np.mean(results[method]['tardiness'])\n",
    "    tard_std = np.std(results[method]['tardiness'])\n",
    "    energy = np.mean(results[method]['energy']) / 1e6\n",
    "    energy_std = np.std(results[method]['energy']) / 1e6\n",
    "    print(f\"{method:<25} {late:>5.1f}±{late_std:<5.1f}% {tard:>6.2f}±{tard_std:<6.2f} {energy:>6.2f}±{energy_std:<6.2f}\")\n",
    "\n",
    "# Compare RL vs best baseline\n",
    "rl_late = np.mean(results['RL-PPO (Enhanced)']['late_frac'])\n",
    "best_baseline_name = min([m for m in methods if 'RL' not in m], key=lambda m: np.mean(results[m]['late_frac']))\n",
    "best_late = np.mean(results[best_baseline_name]['late_frac'])\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"KEY COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nRL-PPO (Enhanced): {rl_late*100:.1f}% late\")\n",
    "print(f\"Best Baseline ({best_baseline_name}): {best_late*100:.1f}% late\")\n",
    "\n",
    "if rl_late < best_late:\n",
    "    improvement = (best_late - rl_late) / best_late * 100\n",
    "    print(f\"\\n✅ RL WINS! {improvement:.1f}% improvement over {best_baseline_name}\")\n",
    "else:\n",
    "    gap = (rl_late - best_late) / best_late * 100\n",
    "    print(f\"\\n❌ RL loses by {gap:.1f}% - may need more training or problem restructuring\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LaTeX output\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"LATEX TABLE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "latex = r\"\"\"\\begin{table}[htbp]\n",
    "\\centering\n",
    "\\caption{Enhanced RL GPU Scheduling Results}\n",
    "\\label{tab:enhanced}\n",
    "\\begin{tabular}{lccc}\n",
    "\\toprule\n",
    "\\textbf{Method} & \\textbf{Late (\\%)} & \\textbf{Tardiness} & \\textbf{Energy (MJ)} \\\\\n",
    "\\midrule\n",
    "\"\"\"\n",
    "\n",
    "for method in methods:\n",
    "    late = np.mean(results[method]['late_frac']) * 100\n",
    "    late_std = np.std(results[method]['late_frac']) * 100\n",
    "    tard = np.mean(results[method]['tardiness'])\n",
    "    tard_std = np.std(results[method]['tardiness'])\n",
    "    energy = np.mean(results[method]['energy']) / 1e6\n",
    "    energy_std = np.std(results[method]['energy']) / 1e6\n",
    "    \n",
    "    m_tex = method.replace('_', '\\\\_')\n",
    "    if 'RL' in method:\n",
    "        latex += f\"\\\\textbf{{{m_tex}}} & \\\\textbf{{{late:.1f}$\\\\pm${late_std:.1f}}} & \\\\textbf{{{tard:.2f}$\\\\pm${tard_std:.2f}}} & \\\\textbf{{{energy:.2f}$\\\\pm${energy_std:.2f}}} \\\\\\\\\\n\"\n",
    "    else:\n",
    "        latex += f\"{m_tex} & {late:.1f}$\\\\pm${late_std:.1f} & {tard:.2f}$\\\\pm${tard_std:.2f} & {energy:.2f}$\\\\pm${energy_std:.2f} \\\\\\\\\\n\"\n",
    "\n",
    "latex += r\"\"\"\\bottomrule\n",
    "\\end{tabular}\n",
    "\\end{table}\"\"\"\n",
    "\n",
    "print(latex)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
